{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867812f1-942f-4027-9c8a-bd887527d433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:46:11.203747Z",
     "start_time": "2023-10-06T06:46:11.174974Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/WongKinYiu/yolov7.git /home/jovyan/yolov7\n",
    "\n",
    "# mkdir weights\n",
    "# curl https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt > weights/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef717f8e-ae8a-407b-a41c-2556513e0ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T07:08:11.990331Z",
     "start_time": "2023-10-06T07:08:10.248295Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !python -m ipykernel install --user --name yolov7 --display-name yolov7\n",
    "# !{sys.executable} -m pip install -r requirements.txt --quiet\n",
    "!{sys.executable} -m pip install kfp==1.8.22 --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aba4f838-4271-4749-a6ff-6407cbe377ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:05.725326Z",
     "start_time": "2023-10-06T08:59:05.365724Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp import compiler, dsl, components\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import component\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "# from kfp import compiler\n",
    "# from kfp import dsl\n",
    "# from kfp.dsl import Input, InputPath, Output, OutputPath, Dataset, Model, component\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "918b3a66-5246-4d0d-b82a-9dbace34cc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:06.762835Z",
     "start_time": "2023-10-06T08:59:06.747209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.22'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28cc25ac-15b1-4592-a2cc-81eca35a4ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:07:50.563696Z",
     "start_time": "2023-10-06T09:07:50.530425Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install=['google-cloud-storage', 'numpy', 'chevron', 'tarfile']\n",
    "# )\n",
    "def prepare_labelstudio_data_for_yolo(\n",
    "        # model_name: str,\n",
    "        project: str,\n",
    "        labels: list,\n",
    "        namespace: str,\n",
    "        domain: str,\n",
    "        config_template_url: str,\n",
    "        transfer_weights_url: str,\n",
    "        hyp_file_url: str,\n",
    "        train_frac: float,\n",
    "        validate_frac: float,\n",
    "        workspace_path: OutputPath(str)\n",
    ") -> NamedTuple('YOLOArgs',\n",
    "                [('data_file', str),\n",
    "                 ('config_file', str),\n",
    "                 ('weights_file', str),\n",
    "                 ('hyp_file', str),\n",
    "                 ('names_file', str)]):\n",
    "    '''\n",
    "    Prepares Labelstudio Data for YOLO training\n",
    "    Example weights_url: https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n",
    "    '''\n",
    "\n",
    "    print(f'Prepares Labelstudio Data for training of YOLOv7')\n",
    "    import chevron\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    # import numpy as np\n",
    "    from urllib.parse import urlparse  #, ParseResult\n",
    "    # from random import shuffle\n",
    "    from collections import namedtuple\n",
    "    import tarfile\n",
    "\n",
    "    def create_directory(directory_name, basedir=None):\n",
    "        if basedir is None:\n",
    "            pth = Path(directory_name)\n",
    "        else:\n",
    "            pth = Path(os.path.join(basedir, directory_name))\n",
    "        pth.mkdir(parents=True, exist_ok=True)\n",
    "        return pth.as_posix()\n",
    "\n",
    "    # Directory structure\n",
    "    basedir = create_directory(\"/workspace\")  # /workspace\n",
    "    configdir = create_directory('config', basedir)  # /workspace/model\n",
    "    datadir = create_directory('dataset', basedir)  # /workspace/dataset\n",
    "    imagesdir = create_directory('images', datadir)  # /workspace/dataset/images\n",
    "    trainimagesdir = create_directory('train', imagesdir)  # /workspace/dataset/images/train\n",
    "    valimagesdir = create_directory('val', imagesdir)  # /workspace/dataset/images/val\n",
    "    testimagesdir = create_directory('test', imagesdir)  # /workspace/dataset/images/test\n",
    "    labelsdir = create_directory('labels', datadir)  # /workspace/dataset/labels\n",
    "    trainlabelsdir = create_directory('train', labelsdir)  # /workspace/dataset/labels/train\n",
    "    vallabelsdir = create_directory('val', labelsdir)  # /workspace/dataset/labels/val\n",
    "    testlabelsdir = create_directory('test', labelsdir)  # /workspace/dataset/labels/test\n",
    "\n",
    "    # Remove lost and found folder\n",
    "    try:\n",
    "        Path.rmdir(\"/workspace/lost+found\")\n",
    "    except:\n",
    "        print(\"Could Not Remove 'lost+found'\")\n",
    "\n",
    "    bucket_name = f\"{namespace}.{domain}\"\n",
    "\n",
    "    print(namespace)\n",
    "    print(domain)\n",
    "    print(bucket_name)\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    my_prefix = f\"label-studio/projects/{project}/results/\"  # the name of the subfolder\n",
    "    blobs = list(bucket.list_blobs(prefix=my_prefix, delimiter='/'))\n",
    "\n",
    "    if len(blobs) == 0:\n",
    "        print(\"NO RESULTS FOUND FOR {}\".format(project))\n",
    "        # output = namedtuple('YOLOArgs', ['data_file', 'config_file', 'transfer_weights_file', 'hyp_file', 'names_file'])\n",
    "        # return output(\"\", \"\", \"\", \"\", \"\")\n",
    "        return\n",
    "\n",
    "    test_frac = 1.0 - train_frac - validate_frac\n",
    "    # shuffle(blobs)\n",
    "    # need to pre-define specific train/val/test indices (avoid risk of leakage - especially with video data)\n",
    "    # This needs to be discussed further. Sampling is challenging.\n",
    "    # shuffle will work for random images that have no temporal relationships provided we ALWAYS keep the same\n",
    "    # test set as a hold out, i.e. don't resample every time we retrain.\n",
    "    m = len(blobs)\n",
    "    train_end = int(train_frac * m)\n",
    "    validate_end = int(validate_frac * m) + train_end\n",
    "\n",
    "    print(f'Number of images: {m}')\n",
    "    print(f'Train set size: {int(train_frac * m)}')\n",
    "    print(f'Validate set size: {int(validate_frac * m)}')\n",
    "    print(f'Test set size: {int(test_frac * m)}')\n",
    "\n",
    "    def save_to_yolo_fmt(file_name, annotation, local_image_folder, local_label_folder):\n",
    "        # <object-class> - integer number of object from 0 to (classes-1)\n",
    "        # <x> <y> <width> <height> - float values relative to width and height of image, it can be equal from (0.0 to 1.0]\n",
    "        # for example: <x> = <absolute_x> / <image_width> or <height> = <absolute_height> / <image_height>\n",
    "        # attention: <x> <y> - are center of rectangle (are not top-left corner)\n",
    "\n",
    "        # file_name is the name of the label studio file to which an annotation is stored (usually a numerical ID)\n",
    "        # annotation is the parsed json content of a label studio annotation file\n",
    "        # local_image/label_folder tells us where to save annotations and images for training\n",
    "\n",
    "        image_data_url = annotation['task']['data']['image']\n",
    "        try:\n",
    "            _, ext = os.path.splitext(image_data_url)\n",
    "            parsed_url = urlparse(image_data_url)\n",
    "            blob = bucket.blob(parsed_url.path[1:])\n",
    "            image_destination = os.path.join(local_image_folder, file_name + ext)\n",
    "            print(image_destination)\n",
    "            blob.download_to_filename(image_destination)\n",
    "        except Exception as err_msg:\n",
    "            print(f\"WARNING: {err_msg}\\n - Could not download {image_data_url}, the traing data will be incomplete.\")\n",
    "            return\n",
    "\n",
    "        with open(os.path.join(local_label_folder, file_name + '.txt'), 'w') as f:\n",
    "            for r in annotation['result']:\n",
    "                scale = 100.0\n",
    "\n",
    "                # Labelstudio has different format depending on original size in result:\n",
    "                if ('original_width' not in r or r['original_width'] == 1) or (\n",
    "                        'original_height' not in r or r['original_height'] == 1):\n",
    "                    scale = 10000.0\n",
    "\n",
    "                w = r['value']['width'] / scale\n",
    "                h = r['value']['height'] / scale\n",
    "                x = (r['value']['x'] + (r['value']['width'] / 2)) / scale\n",
    "                y = (r['value']['y'] + (r['value']['height'] / 2)) / scale\n",
    "\n",
    "                for l in r['value']['rectanglelabels']:\n",
    "                    if l in labels:\n",
    "                        idx = labels.index(l)\n",
    "                        # There are a lot of annotations of very small objects, sort them out\n",
    "                        if (w > 0.005) and (h > 0.005):\n",
    "                            f.write(f'{idx} {x} {y} {w} {h}\\n')\n",
    "                            print(f'Adding:   {idx} {x} {y} {w} {h}')\n",
    "                        else:\n",
    "                            print(f'Skipping: {idx} {x} {y} {w} {h}')\n",
    "\n",
    "    print(trainimagesdir)\n",
    "    print(valimagesdir)\n",
    "    print(testimagesdir)\n",
    "    print(trainlabelsdir)\n",
    "    print(vallabelsdir)\n",
    "    print(testlabelsdir)\n",
    "\n",
    "    for i, blob in enumerate(blobs):\n",
    "        if (blob.name != my_prefix):  # ignoring the subfolder itself\n",
    "            if i < train_end:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to train')\n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, trainimagesdir, trainlabelsdir)\n",
    "            elif i < validate_end:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to val')\n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, valimagesdir, vallabelsdir)\n",
    "            else:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to test')\n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, testimagesdir, testlabelsdir)\n",
    "\n",
    "    # DATA\n",
    "    print(f\"Writing DATA file\")\n",
    "    data_file = os.path.join(configdir, f'data.yaml')\n",
    "    print(f\"- DST: {data_file}\")\n",
    "\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    names_file = os.path.join(datadir, \"object.names\")\n",
    "    train_data_file = os.path.join(datadir, \"train.txt\")\n",
    "    val_data_file = os.path.join(datadir, \"val.txt\")\n",
    "    test_data_file = os.path.join(datadir, \"test.txt\")\n",
    "\n",
    "    with open(data_file, 'w') as out:\n",
    "        out.write(f'train: {train_data_file}\\n')\n",
    "        out.write(f'val: {val_data_file}\\n')\n",
    "        out.write(f'test: {test_data_file}\\n')\n",
    "        out.write(f'nc: {num_classes}\\n')\n",
    "        out.write(f'names: [{\", \".join(labels)}]')\n",
    "\n",
    "    with open(names_file, 'w') as out:\n",
    "        for l in labels:\n",
    "            out.write(l + '\\n')\n",
    "\n",
    "    # /workspace/dataset/train.txt\n",
    "    with open(train_data_file, 'w') as out:\n",
    "        for f in os.listdir(trainimagesdir):\n",
    "            out.write(f'{os.path.join(trainimagesdir, f)}\\n')\n",
    "\n",
    "    # /workspace/dataset/val.txt\n",
    "    with open(val_data_file, 'w') as out:\n",
    "        for f in os.listdir(valimagesdir):\n",
    "            out.write(f'{os.path.join(valimagesdir, f)}\\n')\n",
    "\n",
    "    # /workspace/dataset/test.txt\n",
    "    with open(test_data_file, 'w') as out:\n",
    "        for f in os.listdir(testimagesdir):\n",
    "            out.write(f'{os.path.join(testimagesdir, f)}\\n')\n",
    "\n",
    "    # CONFIG (from GCS)\n",
    "    print(f\"Writing YOLOv7 CONFIG file\")\n",
    "    print(f\"- SRC: {config_template_url}\")\n",
    "    custom_config_file = os.path.join(configdir, \"config.yaml\")\n",
    "    print(f\"- DST: {custom_config_file}\")\n",
    "    parsed_url = urlparse(config_template_url)\n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    with open(custom_config_file, 'w') as fout:\n",
    "        parsed_config = chevron.render(blob.download_as_string().decode(),\n",
    "                                       {\"num_classes\": num_classes})\n",
    "        # to configure anything else, create a copy, modify, and reference.\n",
    "        # full parameterization would likely be more confusing than helpful\n",
    "        fout.write(parsed_config)\n",
    "        print(parsed_config)\n",
    "\n",
    "    # HYP FILE (from GCS)\n",
    "    print(\"Copying HYP file\")\n",
    "    print(f\"- SRC: {hyp_file_url}\")\n",
    "    hyp_file = os.path.join(configdir, \"hyp.yaml\")\n",
    "    print(f\"- DST: {hyp_file}\")\n",
    "    parsed_url = urlparse(hyp_file_url)\n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    blob.download_to_filename(hyp_file)\n",
    "\n",
    "    # PRETRAINED WEIGHTS (from GCS)\n",
    "    transfer_weights_file = \"\"\n",
    "    if transfer_weights_url:\n",
    "        print(f\"Copying weights for transfer learning\")\n",
    "        print(f\"- SRC: {transfer_weights_url}\")\n",
    "        transfer_weights_file = os.path.join(configdir, 'transfer_weights.pt')\n",
    "        print(f\"- DST: {transfer_weights_file}\")\n",
    "        parsed_url = urlparse(transfer_weights_url)\n",
    "        blob = bucket.blob(parsed_url.path[1:])\n",
    "        blob.download_to_filename(transfer_weights_file)\n",
    "\n",
    "    # COMPRESS WORKSPACE\n",
    "    def make_tarfile(output_filename, source_dir):\n",
    "        with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "            tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "\n",
    "    make_tarfile(workspace_path, basedir)\n",
    "\n",
    "    output = namedtuple('YOLOArgs', ['data_file', 'config_file', 'transfer_weights_file', 'hyp_file', 'names_file'])\n",
    "    return output(data_file, custom_config_file, transfer_weights_file, hyp_file, names_file)\n",
    "\n",
    "\n",
    "prepare_labelstudio_data_for_yolo_op = components.create_component_from_func(\n",
    "    prepare_labelstudio_data_for_yolo,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['google-cloud-storage', 'numpy', 'chevron'],\n",
    "    output_component_file='prepare_labelstudio_data_for_yolo.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf4d974-dc58-46ac-ad38-08e5bbc8f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(model_name: str,\n",
    "             workspace: InputPath(str),\n",
    "             weights: str,\n",
    "             data: str,\n",
    "             config: str,\n",
    "             hyp: str,\n",
    "             batch_size: int,\n",
    "             img_size_train: int,\n",
    "             img_size_test: int,\n",
    "             epochs: int):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Train YOLOv7',\n",
    "        image='gcr.io/teknoir/yolov7-training:main-amd64',  # nvidia pytorch base image is VERY large\n",
    "        command=['sh', '-c'],\n",
    "        arguments=[f'''mkdir -p /workspace && tar -xzvf /workspace.tgz -C / && \n",
    "            echo \"WEIGHTS: {weights}\" &&\n",
    "            echo \"DATA: {data}\" &&\n",
    "            echo \"CONFIG: {config}\" &&\n",
    "            echo \"HYP: {hyp}\" &&\n",
    "            python3 train.py --workers 0 --device 0 --batch-size {batch_size} --data {data} --img {img_size_train} {img_size_test} --cfg {config} --name {model_name} --weights {weights} --hyp {hyp} --epochs {epochs} --project=/workspace --name=model --exist-ok'''],\n",
    "        container_kwargs={'env': [V1EnvVar('MODEL_NAME', model_name)]},\n",
    "        artifact_argument_paths=[workspace],\n",
    "        file_outputs={\n",
    "            'best_model': '/workspace/model/weights/best.pt',\n",
    "            'last_model': '/workspace/model/weights/last.pt',\n",
    "            'names': '/workspace/dataset/object.names',\n",
    "            'data': '/workspace/config/data.yaml',\n",
    "            'config': '/workspace/config/config.yaml',\n",
    "            'hyp': '/workspace/config/hyp.yaml',\n",
    "            'F1_curve_image': '/workspace/model/F1_curve.png',\n",
    "            'P_curve_image': '/workspace/model/P_curve.png',\n",
    "            'PR_curve_image': '/workspace/model/PR_curve.png',\n",
    "            'R_curve_image': '/workspace/model/R_curve.png',\n",
    "            'results_txt': '/workspace/model/results.txt',\n",
    "            'results_image': '/workspace/model/results.png',\n",
    "            'confusion_matrix_image': '/workspace/model/confusion_matrix.png',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c507c30-f2b3-4778-90ab-794e3a6668ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:07:50.887163Z",
     "start_time": "2023-10-06T09:07:50.876420Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install=['google-cloud-storage', 'six']\n",
    "# )\n",
    "def save_yolo_model(model_name: str,\n",
    "                    namespace: str,\n",
    "                    domain: str,\n",
    "                    description: str,\n",
    "                    best_model: InputPath(str),\n",
    "                    last_model: InputPath(str),\n",
    "                    names: InputPath(str),\n",
    "                    data: InputPath(str),\n",
    "                    config: InputPath(str),\n",
    "                    hyp: InputPath(str),\n",
    "                    f1_curve: InputPath(str),\n",
    "                    p_curve: InputPath(str),\n",
    "                    pr_curve: InputPath(str),\n",
    "                    r_curve: InputPath(str),\n",
    "                    results_txt: InputPath(str),\n",
    "                    results_image: InputPath(str),\n",
    "                    confusion_matrix_image: InputPath(str)):\n",
    "    # -> NamedTuple('YOLOModel', [('outputs', dict)]):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from collections import namedtuple\n",
    "    # from pathlib import Path\n",
    "\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))\n",
    "\n",
    "    bucket_name = f\"{namespace}.{domain}\"\n",
    "    destination_prefix = os.path.join('models', model_name)\n",
    "\n",
    "    # 'yolov7-tiny.weights', 'yolov7-tiny.cfg', 'object.names', 'confusion_matrix.png',\n",
    "    # 'F1_curve.png', 'P_curve.png', 'PR_curve.png', 'R_curve.png', 'results.png', \n",
    "    # 'hyp.yaml', 'opt.yaml', ...\n",
    "\n",
    "    #Upload best weights file (runs/train/<model name>/weights/best.pt)\n",
    "    upload_blob(bucket_name, best_model, os.path.join(destination_prefix, \"best_weights.pt\"))\n",
    "\n",
    "    #Upload last weights file as well\n",
    "    upload_blob(bucket_name, last_model, os.path.join(destination_prefix, \"last_weights.pt\"))\n",
    "\n",
    "    #Upload object names - eventually deprecate in favor of data.yaml\n",
    "    upload_blob(bucket_name, names, os.path.join(destination_prefix, \"object.names\"))\n",
    "\n",
    "    #Upload data.yaml\n",
    "    upload_blob(bucket_name, data, os.path.join(destination_prefix, \"training_data.yaml\"))\n",
    "\n",
    "    #Upload config.yaml\n",
    "    upload_blob(bucket_name, config, os.path.join(destination_prefix, \"training_config.yaml\"))\n",
    "\n",
    "    #Upload hyp.yaml\n",
    "    upload_blob(bucket_name, hyp, os.path.join(destination_prefix, \"training_hyp.yaml\"))\n",
    "    \n",
    "    #Upload\n",
    "    upload_blob(bucket_name, f1_curve, os.path.join(destination_prefix, \"F1_curve.png\"))\n",
    "    upload_blob(bucket_name, p_curve, os.path.join(destination_prefix, \"P_curve.png\"))\n",
    "    upload_blob(bucket_name, pr_curve, os.path.join(destination_prefix, \"PR_curve.png\"))\n",
    "    upload_blob(bucket_name, r_curve, os.path.join(destination_prefix, \"R_curve.png\"))\n",
    "    upload_blob(bucket_name, results_txt, os.path.join(destination_prefix, \"results.txt\"))\n",
    "    upload_blob(bucket_name, results_image, os.path.join(destination_prefix, \"results.png\"))\n",
    "    upload_blob(bucket_name, confusion_matrix_image, os.path.join(destination_prefix, \"confusion_matrix.png\"))\n",
    "\n",
    "    catalog_info_t = f'''\n",
    "---\n",
    "apiVersion: backstage.io/v1alpha1\n",
    "kind: Component\n",
    "metadata:\n",
    "  title: {model_name}\n",
    "  name: {model_name}\n",
    "  namespace: {namespace}\n",
    "  description: {description}\n",
    "  annotations:\n",
    "    'teknoir.org/model-registry/f1_curve': {os.path.join(destination_prefix, \"F1_curve.png\")}\n",
    "    'teknoir.org/model-registry/p_curve': {os.path.join(destination_prefix, \"P_curve.png\")}\n",
    "    'teknoir.org/model-registry/pr_curve': {os.path.join(destination_prefix, \"PR_curve.png\")}\n",
    "    'teknoir.org/model-registry/r_curve': {os.path.join(destination_prefix, \"R_curve.png\")}\n",
    "    'teknoir.org/model-registry/results_txt': {os.path.join(destination_prefix, \"results.txt\")}\n",
    "    'teknoir.org/model-registry/results': {os.path.join(destination_prefix, \"results.png\")}\n",
    "    'teknoir.org/model-registry/confusion_matrix': {os.path.join(destination_prefix, \"confusion_matrix.png\")}\n",
    "spec:\n",
    "  type: model\n",
    "  owner: group:{namespace}/{namespace}\n",
    "  lifecycle: experimental\n",
    "  system: system:{namespace}/{namespace}\n",
    "'''\n",
    "    with open(\"catalog-info.yaml\", 'w') as f:\n",
    "        f.write(catalog_info_t)\n",
    "    upload_blob(bucket_name, \"catalog-info.yaml\", os.path.join(destination_prefix, \"catalog-info.yaml\"))\n",
    "\n",
    "\n",
    "save_yolo_model_op = components.create_component_from_func(\n",
    "    save_yolo_model,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['google-cloud-storage', 'six'],\n",
    "    output_component_file='save_yolo_model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43e198a9-04c4-41e2-9bf5-a685bf8f20e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:11:14.149915Z",
     "start_time": "2023-10-06T09:11:14.141897Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import kubernetes as k8s\n",
    "from kubernetes.client.models import V1EnvVar, V1ContainerPort\n",
    "\n",
    "pipeline_name = 'Train YOLOv7 with Labelstudio and 1 T4 GPU'\n",
    "pipeline_description = 'A pipeline to train YOLOv7 on a custom data set from Teknoir Labelstudio. Transfer learn from \"transfer_weights.\"'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=pipeline_name, description=pipeline_description)\n",
    "def train_yolov7(model_name: str,\n",
    "                 labelstudio_project: str,\n",
    "                 labels: list,\n",
    "                 namespace: str,\n",
    "                 domain: str,\n",
    "                 config_template_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/cfg/yolov7-tiny-config-template.yaml',\n",
    "                 transfer_weights_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/weights/yolov7_training.pt',\n",
    "                 hyp_file_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/hyp/hyp.scratch.tiny.yaml',\n",
    "                 train_frac: float = 0.7,\n",
    "                 # fraction of samples to use for training (test_frac = 1 - train_frac - validate_frac)\n",
    "                 validate_frac: float = 0.2,\n",
    "                 # fraction of samples to use for validation (test_frac = 1 - train_frac - validate_frac)\n",
    "                 batch_size: int = 16,\n",
    "                 img_size_train: int = 416,\n",
    "                 img_size_test: int = 416,\n",
    "                 epochs: int = 300,\n",
    "                 ):\n",
    "    \"\"\"Train YOLOv7 with Labelstudio\"\"\"\n",
    "\n",
    "    gpu_instance_type = \"nvidia-tesla-t4\"  # https://cloud.google.com/compute/docs/gpus\n",
    "\n",
    "    prepare = prepare_labelstudio_data_for_yolo_op(\n",
    "        project=labelstudio_project,\n",
    "        labels=labels,\n",
    "        namespace=namespace,\n",
    "        domain=domain,\n",
    "        config_template_url=config_template_url,\n",
    "        transfer_weights_url=transfer_weights_url,\n",
    "        hyp_file_url=hyp_file_url,\n",
    "        train_frac=train_frac,\n",
    "        validate_frac=validate_frac\n",
    "    )\n",
    "    prepare.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "\n",
    "    train = train_op(\n",
    "        workspace=kfp.dsl.InputArgumentPath(argument=prepare.outputs['workspace'], path='/workspace.tgz'),\n",
    "        model_name=model_name,\n",
    "        weights=prepare.outputs['weights_file'],\n",
    "        data=prepare.outputs['data_file'],\n",
    "        config=prepare.outputs['config_file'],\n",
    "        hyp=prepare.outputs['hyp_file'],\n",
    "        batch_size=batch_size,\n",
    "        img_size_train=img_size_train,\n",
    "        img_size_test=img_size_test,\n",
    "        epochs=epochs)\n",
    "    train.after(prepare) \\\n",
    "        .add_port(V1ContainerPort(container_port=8099)) \\\n",
    "        .add_port(V1ContainerPort(container_port=8079)) \\\n",
    "        .set_gpu_limit(1) \\\n",
    "        .add_node_selector_constraint('cloud.google.com/gke-accelerator', gpu_instance_type) \\\n",
    "        .execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "\n",
    "    save_model = save_yolo_model_op(\n",
    "        model_name=model_name,\n",
    "        namespace=namespace,\n",
    "        domain=domain,\n",
    "        description=pipeline_description,\n",
    "        best_model=train.outputs['best_model'],\n",
    "        last_model=train.outputs['last_model'],\n",
    "        names=train.outputs['names'],\n",
    "        data=train.outputs['data'],\n",
    "        config=train.outputs['config'],\n",
    "        hyp=train.outputs['hyp'],\n",
    "        f1_curve=train.outputs['F1_curve_image'],\n",
    "        p_curve=train.outputs['P_curve_image'],\n",
    "        pr_curve=train.outputs['PR_curve_image'],\n",
    "        r_curve=train.outputs['R_curve_image'],\n",
    "        results_txt=train.outputs['results_txt'],\n",
    "        results_image=train.outputs['results_image'],\n",
    "        confusion_matrix_image=train.outputs['confusion_matrix_image']\n",
    "    ).after(train)\n",
    "    save_model.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927c619e-85ee-4b6d-a8ad-d85d8706bdf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:11:15.109557Z",
     "start_time": "2023-10-06T09:11:15.000532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "client = kfp.Client(namespace='teknoir')\n",
    "# 5h timeout\n",
    "pipeline_conf = kfp.dsl.PipelineConf().set_timeout(3600 * 5).set_image_pull_policy(policy=\"Always\")\n",
    "workflow = kfp.compiler.Compiler().compile(pipeline_func=train_yolov7,\n",
    "                                           package_path=\"train_yolov7.yaml\",\n",
    "                                           pipeline_conf=pipeline_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99bcc356-7992-4a64-ba88-b1a3f0744c82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "pipeline_version_file = pipeline_file = 'train_yolov7.yaml'\n",
    "\n",
    "client = kfp.Client(namespace='teknoir')\n",
    "# 5h timeout\n",
    "pipeline_conf = kfp.dsl.PipelineConf().set_timeout(3600 * 5).set_image_pull_policy(policy=\"Always\")\n",
    "workflow = kfp.compiler.Compiler().compile(pipeline_func=train_yolov7,\n",
    "                                           package_path=pipeline_file,\n",
    "                                           pipeline_conf=pipeline_conf)\n",
    "\n",
    "filter = json.dumps({'predicates': [{'key': 'name',\n",
    "                                     'op': 1,\n",
    "                                     'string_value': pipeline_name}]})\n",
    "pipelines = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if not pipelines.pipelines:\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file,\n",
    "                                                       name=pipeline_name,\n",
    "                                                       description=pipeline_description)\n",
    "else:\n",
    "    pipeline_version_name = pipeline_name + f' - {str(uuid.uuid4())[:6]}'\n",
    "    pipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,\n",
    "                                                                       name=pipeline_version_name,\n",
    "                                                                       pipelineid=pipelines.pipelines[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50e42d-9007-4160-b695-3ca9d38f9112",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1784f-572c-4a50-ac20-af4733a4e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
