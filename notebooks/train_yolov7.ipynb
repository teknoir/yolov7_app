{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "29e01381-1e7a-47bf-a999-8189c9b43a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/WongKinYiu/yolov7.git /home/jovyan/yolov7\n",
    "\n",
    "# mkdir weights\n",
    "# curl https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt > weights/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "aba4f838-4271-4749-a6ff-6407cbe377ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp import compiler, dsl, components\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "20a143f2-e304-4202-a29d-3693a1b2e658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASE_DIR = \"/workspace\"\n",
    "# WEIGHTS = f\"{BASE_DIR}/weights/yolov7_training.pt\"\n",
    "\n",
    "# print(f\"BASE_DIR: {os.getcwd()}\")\n",
    "# print(f\"WEIGHTS: {WEIGHTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "ef717f8e-ae8a-407b-a41c-2556513e0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m ipykernel install --user --name yolov7 --display-name yolov7\n",
    "!{sys.executable} -m pip install -r requirements.txt --quiet\n",
    "!{sys.executable} -m pip install kfp --upgrade --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "28cc25ac-15b1-4592-a2cc-81eca35a4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labelstudio_data_for_yolo(\n",
    "    model_name: str,\n",
    "    project: str,\n",
    "    labels: list,\n",
    "    namespace: str,\n",
    "    domain: str,\n",
    "    config_template_url: str,\n",
    "    transfer_weights_url: str, \n",
    "    hyp_file_url: str,\n",
    "    train_frac: float,\n",
    "    validate_frac: float) -> NamedTuple('YOLOArgs',\n",
    "                                        [('data_file', str),\n",
    "                                         ('config_file', str),\n",
    "                                         ('weights_file', str),\n",
    "                                         ('hyp_file', str),\n",
    "                                         ('names_file', str)]):\n",
    "    \n",
    "    '''\n",
    "    Prepares Labelstudio Data for YOLO training\n",
    "    \n",
    "    Example weights_url: https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n",
    "    '''\n",
    "    \n",
    "    print(f'Prepares Labelstudio Data for training of YOLOv7')\n",
    "    import chevron\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    from urllib.parse import urlparse, ParseResult\n",
    "    #from random import shuffle\n",
    "    from collections import namedtuple\n",
    "\n",
    "    def create_directory(directory_name, basedir=None):\n",
    "        if basedir is None:\n",
    "            pth = Path(directory_name)\n",
    "        else:\n",
    "            pth = Path(os.path.join(basedir, directory_name))\n",
    "        pth.mkdir(parents=True, exist_ok=True)\n",
    "        return pth.as_posix()\n",
    "    \n",
    "    # Directory structure\n",
    "    basedir = create_directory(\"/workspace\")              # BASE\n",
    "    configdir = create_directory('config', basedir)       # BASE/model\n",
    "    datadir = create_directory('dataset', basedir)        # BASE/data\n",
    "    imagesdir = create_directory('images', datadir)       # BASE/dataset/images\n",
    "    trainimagesdir = create_directory('train', imagesdir) # BASE/dataset/images/train\n",
    "    valimagesdir = create_directory('val', imagesdir)     # BASE/dataset/images/val\n",
    "    testimagesdir = create_directory('test', imagesdir)   # BASE/dataset/images/test\n",
    "    labelsdir = create_directory('labels', datadir)       # BASE/dataset/labels\n",
    "    trainlabelsdir = create_directory('train', labelsdir) # BASE/dataset/labels/train\n",
    "    vallabelsdir = create_directory('val', labelsdir)     # BASE/dataset/labels/val\n",
    "    testlabelsdir = create_directory('test', labelsdir)   # BASE/dataset/labels/test\n",
    "    modeldir = create_directory('model', basedir)\n",
    "    \n",
    "    bucket_name = f\"{namespace}.{domain}\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "    my_prefix = f\"label-studio/projects/{project}/results/\" # the name of the subfolder\n",
    "    blobs = list(bucket.list_blobs(prefix = my_prefix, delimiter = '/'))\n",
    "    \n",
    "    if len(blobs) == 0:\n",
    "        print(\"NO RESULTS FOUND FOR {}\".format(project))\n",
    "        output = namedtuple('YOLOArgs', ['data_file', 'config_file', 'transfer_weights_file', 'hyp_file', 'names_file'])\n",
    "        return output(\"\", \"\", \"\", \"\", \"\")\n",
    "    \n",
    "    test_frac = 1.0 - train_frac - validate_frac\n",
    "    # shuffle(blobs)\n",
    "    # need to pre-define specific train/val/test indices (avoid risk of leakage - especially with video data)\n",
    "    # This needs to be discussed further. Sampling is challenging.\n",
    "    # shuffle will work for random images that have no temporal relationships provided we ALWAYS keep the same\n",
    "    # test set as a hold out, i.e. don't resample every time we retrain.\n",
    "    m = len(blobs)\n",
    "    train_end = int(train_frac * m)\n",
    "    validate_end = int(validate_frac * m) + train_end\n",
    "\n",
    "    print(f'Number of images: {m}')\n",
    "    print(f'Train set size: {int(train_frac * m)}')\n",
    "    print(f'Validate set size: {int(validate_frac * m)}')\n",
    "    print(f'Test set size: {int(test_frac * m)}')\n",
    "\n",
    "    def strip_scheme(url):\n",
    "        parsed_result = urlparse(url)\n",
    "        print(parsed_result)\n",
    "        return ParseResult('', parsed_result[1:]).geturl()\n",
    "\n",
    "    def save_to_yolo_fmt(file_name, annotation, local_image_folder, local_label_folder):\n",
    "        # <object-class> - integer number of object from 0 to (classes-1)\n",
    "        # <x> <y> <width> <height> - float values relative to width and height of image, it can be equal from (0.0 to 1.0]\n",
    "        # for example: <x> = <absolute_x> / <image_width> or <height> = <absolute_height> / <image_height>\n",
    "        # attention: <x> <y> - are center of rectangle (are not top-left corner)\n",
    "        \n",
    "        # file_name is the name of the label studio file to which an annotation is stored (usually a numerical ID)\n",
    "        # annotation is the parsed json content of a label studio annotation file\n",
    "        # local_image/label_folder tells us where to save annotations and images for training\n",
    "       \n",
    "        image_data_url = annotation['task']['data']['image']\n",
    "        try:\n",
    "            _, ext = os.path.splitext(image_data_url)\n",
    "            parsed_url = urlparse(image_data_url)\n",
    "            blob = bucket.blob(parsed_url.path[1:])\n",
    "            image_destination = os.path.join(local_image_folder, file_name + ext)\n",
    "            print(image_destination)\n",
    "            blob.download_to_filename(image_destination)\n",
    "        except Exception as err_msg:\n",
    "            print(f\"ERROR: {err_msg}\\n - Could not download {image_data_url}\")\n",
    "            return\n",
    "        \n",
    "        with open(os.path.join(local_label_folder,file_name + '.txt'), 'w') as f:\n",
    "            for r in annotation['result']:\n",
    "                scale = 100.0\n",
    "\n",
    "                # Labelstudio has different format depending on original size in result:\n",
    "                if ('original_width' not in r or r['original_width'] == 1) or ('original_height' not in r or r['original_height'] == 1):\n",
    "                    scale = 10000.0\n",
    "                \n",
    "                w = r['value']['width']/scale\n",
    "                h = r['value']['height']/scale\n",
    "                x = (r['value']['x']+(r['value']['width']/2))/scale\n",
    "                y = (r['value']['y']+(r['value']['height']/2))/scale\n",
    "                \n",
    "                for l in r['value']['rectanglelabels']:\n",
    "                    if l in labels:\n",
    "                        idx = labels.index(l)\n",
    "                        # There are a lot of annotations of very small objects, sort them out\n",
    "                        if (w > 0.005) and (h > 0.005):\n",
    "                            f.write(f'{idx} {x} {y} {w} {h}\\n')\n",
    "                            print(f'Adding:   {idx} {x} {y} {w} {h}')\n",
    "                        else:\n",
    "                            print(f'Skipping: {idx} {x} {y} {w} {h}')\n",
    "    \n",
    "    print(trainimagesdir)\n",
    "    print(valimagesdir)\n",
    "    print(testimagesdir)\n",
    "    \n",
    "    print(trainlabelsdir)\n",
    "    print(vallabelsdir)\n",
    "    print(testlabelsdir)\n",
    "\n",
    "    for i, blob in enumerate(blobs):\n",
    "        if(blob.name != my_prefix): # ignoring the subfolder itself\n",
    "            if i < train_end:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to train')\n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, trainimagesdir, trainlabelsdir)\n",
    "            elif i < validate_end:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to val')\n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, valimagesdir, vallabelsdir)\n",
    "            else:\n",
    "                file_name = blob.name.replace(my_prefix, \"\")\n",
    "                print(f'adding {i}, {file_name} to test') \n",
    "                annotation = json.loads(blob.download_as_string().decode())\n",
    "                save_to_yolo_fmt(file_name, annotation, testimagesdir, testlabelsdir)\n",
    "    \n",
    "    # DATA\n",
    "    print(f\"Writing DATA file\")\n",
    "    data_file = os.path.join(configdir, f'data.yaml')\n",
    "    print(f\"- DST: {data_file}\")\n",
    "    \n",
    "    num_classes = len(labels)\n",
    "    \n",
    "    train_data_file = os.path.join(datadir,\"train.txt\")\n",
    "    val_data_file = os.path.join(datadir,\"val.txt\")\n",
    "    test_data_file = os.path.join(datadir,\"test.txt\")\n",
    "    \n",
    "    with open(data_file, 'w') as out:\n",
    "      out.write(f'train: {train_data_file}\\n')\n",
    "      out.write(f'val: {val_data_file}\\n')\n",
    "      out.write(f'test: {test_data_file}\\n')\n",
    "      out.write(f'nc: {num_classes}\\n')\n",
    "      out.write(f'names: [{\", \".join(labels)}]')\n",
    "    \n",
    "    with open(names_file, 'w') as out:\n",
    "      for l in labels:\n",
    "        out.write(l + '\\n')\n",
    "    \n",
    "    # /workspace/dataset/train.txt\n",
    "    with open(train_data_file, 'w') as out:\n",
    "        for f in os.listdir(trainimagesdir):\n",
    "            out.write(f'{os.path.join(trainimagesdir, f)}\\n')\n",
    "    \n",
    "    # /workspace/dataset/val.txt\n",
    "    with open(val_data_file, 'w') as out:\n",
    "        for f in os.listdir(valimagesdir):\n",
    "            out.write(f'{os.path.join(valimagesdir, f)}\\n')\n",
    "    \n",
    "    # /workspace/dataset/test.txt\n",
    "    with open(test_data_file, 'w') as out:\n",
    "        for f in os.listdir(testimagesdir):\n",
    "            out.write(f'{os.path.join(testimagesdir, f)}\\n')\n",
    "    \n",
    "    # CONFIG (from GCS)\n",
    "    print(f\"Writing YOLOv7 CONFIG file\")\n",
    "    print(f\"- SRC: {config_template_url}\")\n",
    "    custom_config_file = os.path.join(configdir, \"config.yaml\")\n",
    "    print(f\"- DST: {custom_config_file}\")\n",
    "    parsed_url = urlparse(config_template_url)\n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    with open(custom_config_file, 'w') as fout:\n",
    "        parsed_config = chevron.render(blob.download_as_string().decode(), \n",
    "                                       {\"num_classes\": num_classes})\n",
    "        # to configure anything else, create a copy, modify, and reference.\n",
    "        # full parameterization would likely be more confusing than helpful\n",
    "        fout.write(parsed_config)\n",
    "        print(parsed_config)\n",
    "    \n",
    "    # HYP FILE (from GCS)\n",
    "    print(\"Copying HYP file\")\n",
    "    print(f\"- SRC: {hyp_file_url}\")\n",
    "    hyp_file = os.path.join(configdir, \"hyp.yaml\")\n",
    "    print(f\"- DST: {hyp_file}\")\n",
    "    parsed_url = urlparse(hyp_file_url) \n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    blob.download_to_filename(hyp_file)\n",
    "    \n",
    "    # PRETRAINED WEIGHTS (from GCS)\n",
    "    transfer_weights_file = \"\"\n",
    "    if transfer_weights_url:\n",
    "        print(f\"Copying weights for transfer learning\")\n",
    "        print(f\"- SRC: {transfer_weights_url}\")\n",
    "        transfer_weights_file = os.path.join(configdir, 'transfer_weights.pt')\n",
    "        print(f\"- DST: {transfer_weights_file}\")\n",
    "        parsed_url = urlparse(transfer_weights_url) \n",
    "        blob = bucket.blob(parsed_url.path[1:])\n",
    "        blob.download_to_filename(transfer_weights_file)\n",
    "    \n",
    "    output = namedtuple('YOLOArgs', ['data_file', 'config_file', 'transfer_weights_file', 'hyp_file', 'names_file'])\n",
    "    return output(data_file, custom_config_file, transfer_weights_file, hyp_file, names_file)\n",
    "\n",
    "prepare_labelstudio_data_for_yolo_op = components.create_component_from_func(\n",
    "    prepare_labelstudio_data_for_yolo,\n",
    "    packages_to_install=['google-cloud-storage', 'numpy', 'chevron'],\n",
    "    output_component_file='prepare_labelstudio_data_for_yolo.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "5db7b048-f99e-41c3-bf7c-59fab08fd0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yolo_data(\n",
    "    model_name: str,\n",
    "    namespace: str,\n",
    "    domain: str):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "\n",
    "        print(\n",
    "            \"File {} uploaded to {}.\".format(\n",
    "                source_file_name, destination_blob_name\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    bucket_name = f\"{namespace}.{domain}\"    \n",
    "    destination_yolo_data = os.path.join('yolo_data', f'{model_name}.tar.gz')\n",
    "\n",
    "    basedir = '/workspace'\n",
    "    yolo_data = os.path.join(basedir, f'{model_name}.tar.gz')\n",
    "\n",
    "    upload_blob(bucket_name, yolo_data, destination_yolo_data)\n",
    "\n",
    "save_yolo_data_op = components.create_component_from_func(\n",
    "    save_yolo_data,\n",
    "    packages_to_install=['google-cloud-storage', 'six'],\n",
    "    output_component_file='save_yolo_data.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "3c507c30-f2b3-4778-90ab-794e3a6668ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_yolo_model(model_name: str,\n",
    "                    namespace: str,\n",
    "                    domain: str) -> NamedTuple('YOLOModel',[('outputs', dict)]):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from collections import namedtuple\n",
    "    from pathlib import Path\n",
    "\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))\n",
    "    \n",
    "    bucket_name = f\"{namespace}.{domain}\"\n",
    "    destination_prefix = os.path.join('models', model_name)\n",
    "    \n",
    "    outputs = {}\n",
    "    \n",
    "    # 'yolov7-tiny.weights', 'yolov7-tiny.cfg', 'object.names', 'confusion_matrix.png',\n",
    "    # 'F1_curve.png', 'P_curve.png', 'PR_curve.png', 'R_curve.png', 'results.png', \n",
    "    # 'hyp.yaml', 'opt.yaml', ...\n",
    "    model_training_results = os.path.join(\"/workspace\",\"model\")\n",
    "    for training_output_file in os.listdir(model_training_results):\n",
    "        src = os.path.join(model_training_results, training_output_file)\n",
    "        if os.path.isfile(src): # skip folders to avoid uploading all weights\n",
    "            dst = os.path.join(destination_prefix, training_output_file)\n",
    "            upload_blob(bucket_name, src, dst)\n",
    "            outputs[src] = dst\n",
    "    \n",
    "    #Upload best weights file (runs/train/<model name>/weights/best.pt)\n",
    "    src = os.path.join(\"/workspace\", \"model\", \"weights\", \"best.pt\")\n",
    "    dst = os.path.join(destination_prefix, \"best_weights.pt\")\n",
    "    upload_blob(bucket_name, src, dst)\n",
    "    outputs[src] = dst\n",
    "    \n",
    "    #Upload last weights file as well\n",
    "    src = os.path.join(\"/workspace\", \"model\", \"weights\", \"last.pt\")\n",
    "    dst = os.path.join(destination_prefix, \"last_weights.pt\")\n",
    "    upload_blob(bucket_name, src, dst)\n",
    "    outputs[src] = dst\n",
    "    \n",
    "    output = namedtuple('YOLOModel', ['outputs'])\n",
    "    return output(outputs)\n",
    "\n",
    "save_yolo_model_op = components.create_component_from_func(\n",
    "    save_yolo_model,\n",
    "    packages_to_install=['google-cloud-storage', 'six'],\n",
    "    output_component_file='save_yolo_model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "43e198a9-04c4-41e2-9bf5-a685bf8f20e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kubernetes as k8s\n",
    "from kubernetes.client.models import V1EnvVar, V1ContainerPort\n",
    "\n",
    "pipeline_name = 'Train YOLOv7 with Labelstudio and 1 T4 GPU'\n",
    "pipeline_description = 'A pipeline to train YOLOv7 on a custom data set from Teknoir Labelstudio. Transfer learn from \"transfer_weights.\"'\n",
    "\n",
    "@dsl.pipeline(name = pipeline_name, description = pipeline_description)\n",
    "def train_yolov7(model_name, labelstudio_project, labels, namespace, domain,\n",
    "                 config_template_url='gs://teknoir-ai.teknoir.cloud/yolov7/cfg/yolov7-tiny-config-template.yaml',\n",
    "                 transfer_weights_url='gs://teknoir-ai.teknoir.cloud/yolov7/weights/yolov7_training.pt',\n",
    "                 hyp_file_url='gs://teknoir-ai.teknoir.cloud/yolov7/hyp/hyp.scratch.tiny.yaml',\n",
    "                 train_frac=0.7, # fraction of samples to use for training (test_frac = 1 - train_frac - validate_frac)\n",
    "                 validate_frac=0.2, # fraction of samples to use for validation (test_frac = 1 - train_frac - validate_frac)\n",
    "                 batch_size=32,\n",
    "                 img_size_train=640,\n",
    "                 img_size_test=640,\n",
    "                 epochs=300,\n",
    "                 disk_space=\"10000Mi\"):\n",
    "\n",
    "    \"\"\"Train YOLOv7 with Labelstudio\"\"\"\n",
    "    \n",
    "    mount_folder = \"/workspace\"\n",
    "    gpu_instance_type = \"nvidia-tesla-t4\" # https://cloud.google.com/compute/docs/gpus\n",
    "    \n",
    "    # A working directory between steps\n",
    "    wkdirop = dsl.VolumeOp(\n",
    "        name=f'Create training workspace',\n",
    "        resource_name=model_name,\n",
    "        size=disk_space,\n",
    "        modes=dsl.VOLUME_MODE_RWO\n",
    "    )\n",
    "    \n",
    "    prepare = prepare_labelstudio_data_for_yolo_op(\n",
    "        model_name=model_name,\n",
    "        project=labelstudio_project,\n",
    "        labels=labels,\n",
    "        namespace=namespace,\n",
    "        domain=domain,\n",
    "        config_template_url=config_template_url,\n",
    "        transfer_weights_url=transfer_weights_url,\n",
    "        hyp_file_url=hyp_file_url,\n",
    "        train_frac=train_frac,\n",
    "        validate_frac=validate_frac\n",
    "    ).add_pvolumes({mount_folder: wkdirop.volume})\n",
    "\n",
    "    compress = dsl.ContainerOp(\n",
    "        name=\"Compress YOLO data\",\n",
    "        image=\"alpine\",\n",
    "        command=[\"sh\", \"-c\"],\n",
    "        arguments=[f'apk add tree && tree {mount_folder} && cd {mount_folder} && touch {model_name}.tar.gz && tar -czvf {model_name}.tar.gz --exclude={model_name}.tar.gz .'],\n",
    "        pvolumes={mount_folder: wkdirop.volume}\n",
    "    ).after(prepare)\n",
    "    \n",
    "    save_data = save_yolo_data_op(\n",
    "        model_name=model_name,\n",
    "        namespace=namespace,\n",
    "        domain=domain).after(compress).add_pvolumes({mount_folder: wkdirop.volume})\n",
    "    \n",
    "    train = dsl.ContainerOp(\n",
    "        name='Train YOLOv7',\n",
    "        image='gcr.io/teknoir/yolov7-training:main-amd64', # nvidia pytorch base image is VERY large\n",
    "        command=['sh', '-c'],\n",
    "        arguments=[\"echo 'WEIGHTS: {weights_file}' && \\\n",
    "            echo 'DATA: {data_file}' && \\\n",
    "            echo 'CONFIG: {config_file}' && \\\n",
    "            echo 'HYP: {hyp_file}' && \\\n",
    "            python3 train.py --workers 0 --device 0 --batch-size {batch_size} \\\n",
    "            --data {data_file} \\\n",
    "            --img {img_size_train} {img_size_test} \\\n",
    "            --cfg {config_file} \\\n",
    "            --name {model_name} \\\n",
    "            --weights '{weights_file}' \\\n",
    "            --hyp {hyp_file} \\\n",
    "            --epochs {epochs} \\\n",
    "            --project=/workspace --name=model --exist-ok\".format(\n",
    "            batch_size=batch_size,\n",
    "            data_file=prepare.outputs[\"data_file\"],\n",
    "            img_size_train=img_size_train,\n",
    "            img_size_test=img_size_test,\n",
    "            config_file=prepare.outputs[\"config_file\"],\n",
    "            weights_file=prepare.outputs[\"weights_file\"],\n",
    "            hyp_file=prepare.outputs[\"hyp_file\"],\n",
    "            epochs=epochs,\n",
    "            model_name=model_name\n",
    "        )],\n",
    "        pvolumes={mount_folder: wkdirop.volume},\n",
    "        # pass in k8s container kwargs\n",
    "        container_kwargs={'env': [V1EnvVar('MODEL_NAME', model_name)]}\n",
    "    ).after(prepare) \\\n",
    "        .add_port(V1ContainerPort(container_port=8099)) \\\n",
    "        .add_port(V1ContainerPort(container_port=8079)) \\\n",
    "        .set_gpu_limit(1) \\\n",
    "        .add_node_selector_constraint('cloud.google.com/gke-accelerator', \n",
    "                                      gpu_instance_type)\n",
    " \n",
    "    # @component\n",
    "    # def html_visualization(html_artifact: Output[HTML]):\n",
    "    #    html_content = '<!DOCTYPE html><html><body><h1>Hello world</h1></body></html>'\n",
    "    #    with open(html_artifact.path, 'w') as f:\n",
    "    #        f.write(html_content)\n",
    "        \n",
    "    # Export to ONNX\n",
    "    # python export.py --weights runs/train/{model_name}/weights/best.pt --grid \\\n",
    "    #     --end2end --simplify --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 \\\n",
    "    #     --img-size 640 640 --max-wh 640\n",
    "    # TensorRT export also discussed: https://github.com/WongKinYiu/yolov7\n",
    "\n",
    "    show_after = dsl.ContainerOp(\n",
    "        name=\"Show workspace after training\",\n",
    "        image=\"alpine\",\n",
    "        command=[\"sh\", \"-c\"],\n",
    "        arguments=[f'apk add tree && tree {mount_folder}'],\n",
    "        pvolumes={mount_folder: wkdirop.volume}\n",
    "    ).after(train)\n",
    " \n",
    "    save_model = save_yolo_model_op(\n",
    "        model_name=model_name,\n",
    "        namespace=namespace,\n",
    "        domain=domain\n",
    "    ).add_pvolumes({mount_folder: wkdirop.volume}).after(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "89e85fac-c120-4bee-9dec-2aa74ae34bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_version_file = pipeline_file = 'train_yolov7.yaml'\n",
    "workflow = kfp.compiler.Compiler().compile(pipeline_func=train_yolov7, \n",
    "                                           package_path=pipeline_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "99bcc356-7992-4a64-ba88-b1a3f0744c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "client = kfp.Client(namespace='teknoir')\n",
    "\n",
    "filter = json.dumps({'predicates': [{'key': 'name', \n",
    "                                     'op': 1, \n",
    "                                     'string_value': pipeline_name}]})\n",
    "pipelines = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if not pipelines.pipelines:\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file, \n",
    "                                                       name=pipeline_name, \n",
    "                                                       description=pipeline_description)\n",
    "else:\n",
    "    pipeline_version_name = pipeline_name + f' - {str(uuid.uuid4())[:6]}'\n",
    "    pipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,\n",
    "                                                                       name=pipeline_version_name, \n",
    "                                                                       pipelineid=pipelines.pipelines[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50e42d-9007-4160-b695-3ca9d38f9112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov7",
   "language": "python",
   "name": "yolov7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
