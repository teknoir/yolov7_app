{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "867812f1-942f-4027-9c8a-bd887527d433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T06:46:11.203747Z",
     "start_time": "2023-10-06T06:46:11.174974Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/WongKinYiu/yolov7.git /home/jovyan/yolov7\n",
    "\n",
    "# mkdir weights\n",
    "# curl https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt > weights/yolov7_training.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef717f8e-ae8a-407b-a41c-2556513e0ca5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T07:08:11.990331Z",
     "start_time": "2023-10-06T07:08:10.248295Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts dsl-compile, dsl-compile-v2 and kfp are installed in '/home/jovyan/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# !python -m ipykernel install --user --name yolov7 --display-name yolov7\n",
    "# !{sys.executable} -m pip install -r requirements.txt --quiet\n",
    "python -m pip install kfp==1.8.22 --user --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aba4f838-4271-4749-a6ff-6407cbe377ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:05.725326Z",
     "start_time": "2023-10-06T08:59:05.365724Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import kfp\n",
    "from kfp import compiler, dsl, components\n",
    "from kfp.components import InputPath, OutputPath\n",
    "from typing import NamedTuple\n",
    "from kfp.dsl import component\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "918b3a66-5246-4d0d-b82a-9dbace34cc12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T08:59:06.762835Z",
     "start_time": "2023-10-06T08:59:06.747209Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.22'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kfp.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "28cc25ac-15b1-4592-a2cc-81eca35a4ff5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:07:50.563696Z",
     "start_time": "2023-10-06T09:07:50.530425Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install=['google-cloud-storage', 'numpy', 'chevron', 'tarfile']\n",
    "# )\n",
    "def prepare_labelstudio_data_for_yolo(\n",
    "        project: str,\n",
    "        labels: list,\n",
    "        namespace: str,\n",
    "        domain: str,\n",
    "        config_template_url: str,\n",
    "        transfer_weights_url: str,\n",
    "        hyp_file_url: str,\n",
    "        train_frac: float,\n",
    "        validate_frac: float,\n",
    "        workspace: OutputPath(str)\n",
    ") -> NamedTuple('YOLOArgs',\n",
    "                [('data_file', str),\n",
    "                 ('config_file', str),\n",
    "                 ('weights_file', str),\n",
    "                 ('hyp_file', str),\n",
    "                 ('names_file', str)]):\n",
    "    '''\n",
    "    Prepares Labelstudio Data for YOLO training\n",
    "    Example weights_url: https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7_training.pt\n",
    "    '''\n",
    "\n",
    "    print(f'Prepares Labelstudio Data for training of YOLOv7')\n",
    "\n",
    "    import chevron\n",
    "    import os\n",
    "    import teknoir_labelstudio_sdk\n",
    "    import zipfile\n",
    "    import json\n",
    "    from random import shuffle\n",
    "    from pathlib import Path\n",
    "    from google.cloud import storage\n",
    "    from urllib.parse import urlparse\n",
    "    from collections import namedtuple\n",
    "    import tarfile\n",
    "\n",
    "    def create_directory(directory_name, basedir=None):\n",
    "        if basedir is None:\n",
    "            pth = Path(directory_name)\n",
    "        else:\n",
    "            pth = Path(os.path.join(basedir, directory_name))\n",
    "        pth.mkdir(parents=True, exist_ok=True)\n",
    "        return pth.as_posix()\n",
    "\n",
    "    # Directory structure\n",
    "    basedir = create_directory(workspace)  # /workspace\n",
    "    configdir = create_directory('config', basedir)  # /workspace/model\n",
    "    datadir = create_directory('dataset', basedir)  # /workspace/dataset\n",
    "    imagesdir = create_directory('images', datadir)  # /workspace/dataset/images\n",
    "    trainimagesdir = create_directory('train', imagesdir)  # /workspace/dataset/images/train\n",
    "    valimagesdir = create_directory('val', imagesdir)  # /workspace/dataset/images/val\n",
    "    testimagesdir = create_directory('test', imagesdir)  # /workspace/dataset/images/test\n",
    "    labelsdir = create_directory('labels', datadir)  # /workspace/dataset/labels\n",
    "    trainlabelsdir = create_directory('train', labelsdir)  # /workspace/dataset/labels/train\n",
    "    vallabelsdir = create_directory('val', labelsdir)  # /workspace/dataset/labels/val\n",
    "    testlabelsdir = create_directory('test', labelsdir)  # /workspace/dataset/labels/test\n",
    "\n",
    "    # Connect to the Label Studio API and check the connection\n",
    "    ls = teknoir_labelstudio_sdk.Client(\n",
    "        extra_headers=teknoir_labelstudio_sdk.jwt_header('kubeflow.training@teknoir.ai'))\n",
    "    print(ls.check_connection())\n",
    "\n",
    "    projects = ls.list_projects(title=project)\n",
    "    p = projects[0]\n",
    "    print(p.parsed_label_config)\n",
    "\n",
    "    yolo_annotations_file_zip = p.export_tasks(export_type=\"YOLO\", download_resources=True,\n",
    "                                                     export_location=f'{workspace}/{project}.zip')\n",
    "\n",
    "    tasks_json_file = p.export_tasks(export_type=\"JSON\", download_resources=True,\n",
    "                                           export_location=f'{workspace}/{project}.json')\n",
    "\n",
    "    with open(tasks_json_file) as f:\n",
    "        tasks = json.load(f)\n",
    "\n",
    "    if len(tasks) == 0:\n",
    "        print(f'NO TASKS FOUND FOR {project}')\n",
    "        exit(0)\n",
    "\n",
    "    ind_list = list(range(len(tasks)))\n",
    "    shuffle(ind_list)\n",
    "    test_frac = 1.0 - train_frac - validate_frac\n",
    "    train_set = ind_list[:int((len(ind_list) + 1) * train_frac)]\n",
    "    test_set = ind_list[int((len(ind_list) + 1) * train_frac):int((len(ind_list) + 1) * (train_frac + test_frac))]\n",
    "    val_set = ind_list[int((len(ind_list) + 1) * (train_frac + test_frac)):]\n",
    "\n",
    "    print(f'Number of tasks: {len(tasks)}')\n",
    "    print(f'Train set size: {len(train_set)}')\n",
    "    print(f'Validate set size: {len(test_set)}')\n",
    "    print(f'Test set size: {len(val_set)}')\n",
    "\n",
    "    bucket_name = f'{namespace}.{domain}'\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    labelstudio_upload_location = 'label-studio/data'\n",
    "\n",
    "    with zipfile.ZipFile(yolo_annotations_file_zip, 'r') as zip_ref:\n",
    "        zip_ref.extractall(f'{workspace}/{project}/yolo')\n",
    "\n",
    "    yolo_labelsdir = f'{workspace}/{project}/yolo/labels'\n",
    "\n",
    "    # DATA SETS\n",
    "    def write_dataset(tasks, set, imagesdir, labelsdir):\n",
    "        for i in set:\n",
    "            task = tasks[i]\n",
    "            if task['data']['image']:\n",
    "                image_path = task['data']['image']\n",
    "                image_name = Path(image_path).name\n",
    "                source_image = f'{labelstudio_upload_location}/{image_path}'\n",
    "                dest_image = f'{imagesdir}/{image_name}'\n",
    "                current_blob = bucket.get_blob(source_image)\n",
    "                current_blob.download_to_filename(dest_image)\n",
    "                print(f\"Image {image_name} was downloaded to {dest_image}.\")\n",
    "\n",
    "                source_labels = f'{yolo_labelsdir}/{Path(image_name).stem}.txt'\n",
    "                dest_labels = f'{labelsdir}/{Path(image_name).stem}.txt'\n",
    "                os.rename(source_labels, dest_labels)\n",
    "                print(f\"Moved labels from {source_labels} to {dest_labels}.\")\n",
    "\n",
    "    # Write training dataset\n",
    "    write_dataset(tasks, train_set, trainimagesdir, trainlabelsdir)\n",
    "\n",
    "    # Write validation dataset\n",
    "    write_dataset(tasks, val_set, valimagesdir, vallabelsdir)\n",
    "\n",
    "    # Write test dataset\n",
    "    write_dataset(tasks, test_set, testimagesdir, testlabelsdir)\n",
    "\n",
    "    # CONFIG\n",
    "    print(f\"Writing CONFIG file\")\n",
    "    data_file = os.path.join(configdir, f'data.yaml')\n",
    "    print(f\"- DST: {data_file}\")\n",
    "\n",
    "    num_classes = len(labels)\n",
    "\n",
    "    names_file = os.path.join(datadir, \"object.names\")\n",
    "    train_data_file = os.path.join(datadir, \"train.txt\")\n",
    "    val_data_file = os.path.join(datadir, \"val.txt\")\n",
    "    test_data_file = os.path.join(datadir, \"test.txt\")\n",
    "\n",
    "    with open(data_file, 'w') as out:\n",
    "        out.write(f'train: {train_data_file}\\n')\n",
    "        out.write(f'val: {val_data_file}\\n')\n",
    "        out.write(f'test: {test_data_file}\\n')\n",
    "        out.write(f'nc: {num_classes}\\n')\n",
    "        out.write(f'names: [{\", \".join(labels)}]')\n",
    "\n",
    "    with open(names_file, 'w') as out:\n",
    "        for l in labels:\n",
    "            out.write(l + '\\n')\n",
    "\n",
    "    # /workspace/dataset/train.txt\n",
    "    with open(train_data_file, 'w') as out:\n",
    "        for f in os.listdir(trainimagesdir):\n",
    "            out.write(f'{os.path.join(trainimagesdir, f)}\\n')\n",
    "\n",
    "    # /workspace/dataset/val.txt\n",
    "    with open(val_data_file, 'w') as out:\n",
    "        for f in os.listdir(valimagesdir):\n",
    "            out.write(f'{os.path.join(valimagesdir, f)}\\n')\n",
    "\n",
    "    # /workspace/dataset/test.txt\n",
    "    with open(test_data_file, 'w') as out:\n",
    "        for f in os.listdir(testimagesdir):\n",
    "            out.write(f'{os.path.join(testimagesdir, f)}\\n')\n",
    "\n",
    "    # CONFIG (from GCS)\n",
    "    print(f\"Writing YOLOv7 CONFIG file\")\n",
    "    print(f\"- SRC: {config_template_url}\")\n",
    "    custom_config_file = os.path.join(configdir, \"config.yaml\")\n",
    "    print(f\"- DST: {custom_config_file}\")\n",
    "    parsed_url = urlparse(config_template_url)\n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    with open(custom_config_file, 'w') as fout:\n",
    "        parsed_config = chevron.render(blob.download_as_string().decode(),\n",
    "                                       {\"num_classes\": num_classes})\n",
    "        # to configure anything else, create a copy, modify, and reference.\n",
    "        # full parameterization would likely be more confusing than helpful\n",
    "        fout.write(parsed_config)\n",
    "        print(parsed_config)\n",
    "\n",
    "    # HYP FILE (from GCS)\n",
    "    print(\"Copying HYP file\")\n",
    "    print(f\"- SRC: {hyp_file_url}\")\n",
    "    hyp_file = os.path.join(configdir, \"hyp.yaml\")\n",
    "    print(f\"- DST: {hyp_file}\")\n",
    "    parsed_url = urlparse(hyp_file_url)\n",
    "    blob = bucket.blob(parsed_url.path[1:])\n",
    "    blob.download_to_filename(hyp_file)\n",
    "\n",
    "    # PRETRAINED WEIGHTS (from GCS)\n",
    "    transfer_weights_file = \"\"\n",
    "    if transfer_weights_url:\n",
    "        print(f\"Copying weights for transfer learning\")\n",
    "        print(f\"- SRC: {transfer_weights_url}\")\n",
    "        transfer_weights_file = os.path.join(configdir, 'transfer_weights.pt')\n",
    "        print(f\"- DST: {transfer_weights_file}\")\n",
    "        parsed_url = urlparse(transfer_weights_url)\n",
    "        blob = bucket.blob(parsed_url.path[1:])\n",
    "        blob.download_to_filename(transfer_weights_file)\n",
    "\n",
    "    output = namedtuple('YOLOArgs', ['data_file', 'config_file', 'transfer_weights_file', 'hyp_file', 'names_file'])\n",
    "    return output(data_file, custom_config_file, transfer_weights_file, hyp_file, names_file)\n",
    "\n",
    "\n",
    "prepare_labelstudio_data_for_yolo_op = components.create_component_from_func(\n",
    "    prepare_labelstudio_data_for_yolo,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['google-cloud-storage', 'numpy', 'chevron', 'git+https://github.com/teknoir/teknoir-labelstudio-sdk.git#egg=teknoir-labelstudio-sdk'],\n",
    "    output_component_file='prepare_labelstudio_data_for_yolo.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1bf4d974-dc58-46ac-ad38-08e5bbc8f42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_op(model_name: str,\n",
    "             workspace_path: InputPath(str),\n",
    "             weights: str,\n",
    "             data: str,\n",
    "             config: str,\n",
    "             hyp: str,\n",
    "             batch_size: int,\n",
    "             img_size_train: int,\n",
    "             img_size_test: int,\n",
    "             epochs: int):\n",
    "    return dsl.ContainerOp(\n",
    "        name='Train YOLOv7',\n",
    "        image='gcr.io/teknoir/yolov7-training:main-amd64',  # nvidia pytorch base image is VERY large\n",
    "        command=['sh', '-c'],\n",
    "        arguments=[f'''echo \"WEIGHTS: {weights}\" &&\n",
    "            echo \"DATA: {data}\" &&\n",
    "            echo \"CONFIG: {config}\" &&\n",
    "            echo \"HYP: {hyp}\" &&\n",
    "            python3 train.py --workers 0 --device 0 --batch-size {batch_size} --data {data} --img {img_size_train} {img_size_test} --cfg {config} --name {model_name} --weights {weights} --hyp {hyp} --epochs {epochs} --project=/workspace --name=model --exist-ok'''],\n",
    "        container_kwargs={'env': [V1EnvVar('MODEL_NAME', model_name)]},\n",
    "        artifact_argument_paths=[workspace_path],\n",
    "        file_outputs={\n",
    "            'best_model': '/workspace/model/weights/best.pt',\n",
    "            'last_model': '/workspace/model/weights/last.pt',\n",
    "            'names': '/workspace/dataset/object.names',\n",
    "            'data': '/workspace/config/data.yaml',\n",
    "            'config': '/workspace/config/config.yaml',\n",
    "            'hyp': '/workspace/config/hyp.yaml',\n",
    "            'F1_curve_image': '/workspace/model/F1_curve.png',\n",
    "            'P_curve_image': '/workspace/model/P_curve.png',\n",
    "            'PR_curve_image': '/workspace/model/PR_curve.png',\n",
    "            'R_curve_image': '/workspace/model/R_curve.png',\n",
    "            'results_txt': '/workspace/model/results.txt',\n",
    "            'results_image': '/workspace/model/results.png',\n",
    "            'confusion_matrix_image': '/workspace/model/confusion_matrix.png',\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c507c30-f2b3-4778-90ab-794e3a6668ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:07:50.887163Z",
     "start_time": "2023-10-06T09:07:50.876420Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# @component(\n",
    "#     packages_to_install=['google-cloud-storage', 'six']\n",
    "# )\n",
    "def save_yolo_model(model_name: str,\n",
    "                    namespace: str,\n",
    "                    domain: str,\n",
    "                    description: str,\n",
    "                    best_model: InputPath(str),\n",
    "                    last_model: InputPath(str),\n",
    "                    names: InputPath(str),\n",
    "                    data: InputPath(str),\n",
    "                    config: InputPath(str),\n",
    "                    hyp: InputPath(str),\n",
    "                    f1_curve: InputPath(str),\n",
    "                    p_curve: InputPath(str),\n",
    "                    pr_curve: InputPath(str),\n",
    "                    r_curve: InputPath(str),\n",
    "                    results_txt: InputPath(str),\n",
    "                    results_image: InputPath(str),\n",
    "                    confusion_matrix_image: InputPath(str)):\n",
    "    # -> NamedTuple('YOLOModel', [('outputs', dict)]):\n",
    "    from google.cloud import storage\n",
    "    import os\n",
    "    from collections import namedtuple\n",
    "    # from pathlib import Path\n",
    "\n",
    "    def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "        \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        print(\"File {} uploaded to {}.\".format(source_file_name, destination_blob_name))\n",
    "\n",
    "    bucket_name = f\"{namespace}.{domain}\"\n",
    "    destination_prefix = os.path.join('models', model_name)\n",
    "\n",
    "    # 'yolov7-tiny.weights', 'yolov7-tiny.cfg', 'object.names', 'confusion_matrix.png',\n",
    "    # 'F1_curve.png', 'P_curve.png', 'PR_curve.png', 'R_curve.png', 'results.png', \n",
    "    # 'hyp.yaml', 'opt.yaml', ...\n",
    "\n",
    "    #Upload best weights file (runs/train/<model name>/weights/best.pt)\n",
    "    upload_blob(bucket_name, best_model, os.path.join(destination_prefix, \"best_weights.pt\"))\n",
    "\n",
    "    #Upload last weights file as well\n",
    "    upload_blob(bucket_name, last_model, os.path.join(destination_prefix, \"last_weights.pt\"))\n",
    "\n",
    "    #Upload object names - eventually deprecate in favor of data.yaml\n",
    "    upload_blob(bucket_name, names, os.path.join(destination_prefix, \"object.names\"))\n",
    "\n",
    "    #Upload data.yaml\n",
    "    upload_blob(bucket_name, data, os.path.join(destination_prefix, \"training_data.yaml\"))\n",
    "\n",
    "    #Upload config.yaml\n",
    "    upload_blob(bucket_name, config, os.path.join(destination_prefix, \"training_config.yaml\"))\n",
    "\n",
    "    #Upload hyp.yaml\n",
    "    upload_blob(bucket_name, hyp, os.path.join(destination_prefix, \"training_hyp.yaml\"))\n",
    "    \n",
    "    #Upload\n",
    "    upload_blob(bucket_name, f1_curve, os.path.join(destination_prefix, \"F1_curve.png\"))\n",
    "    upload_blob(bucket_name, p_curve, os.path.join(destination_prefix, \"P_curve.png\"))\n",
    "    upload_blob(bucket_name, pr_curve, os.path.join(destination_prefix, \"PR_curve.png\"))\n",
    "    upload_blob(bucket_name, r_curve, os.path.join(destination_prefix, \"R_curve.png\"))\n",
    "    upload_blob(bucket_name, results_txt, os.path.join(destination_prefix, \"results.txt\"))\n",
    "    upload_blob(bucket_name, results_image, os.path.join(destination_prefix, \"results.png\"))\n",
    "    upload_blob(bucket_name, confusion_matrix_image, os.path.join(destination_prefix, \"confusion_matrix.png\"))\n",
    "\n",
    "    catalog_info_t = f'''\n",
    "---\n",
    "apiVersion: backstage.io/v1alpha1\n",
    "kind: Component\n",
    "metadata:\n",
    "  title: {model_name}\n",
    "  name: {model_name}\n",
    "  namespace: {namespace}\n",
    "  description: {description}\n",
    "  annotations:\n",
    "    'teknoir.org/model-registry/f1_curve': {os.path.join(destination_prefix, \"F1_curve.png\")}\n",
    "    'teknoir.org/model-registry/p_curve': {os.path.join(destination_prefix, \"P_curve.png\")}\n",
    "    'teknoir.org/model-registry/pr_curve': {os.path.join(destination_prefix, \"PR_curve.png\")}\n",
    "    'teknoir.org/model-registry/r_curve': {os.path.join(destination_prefix, \"R_curve.png\")}\n",
    "    'teknoir.org/model-registry/results_txt': {os.path.join(destination_prefix, \"results.txt\")}\n",
    "    'teknoir.org/model-registry/results': {os.path.join(destination_prefix, \"results.png\")}\n",
    "    'teknoir.org/model-registry/confusion_matrix': {os.path.join(destination_prefix, \"confusion_matrix.png\")}\n",
    "spec:\n",
    "  type: model\n",
    "  owner: group:{namespace}/{namespace}\n",
    "  lifecycle: experimental\n",
    "  system: system:{namespace}/{namespace}\n",
    "'''\n",
    "    with open(\"catalog-info.yaml\", 'w') as f:\n",
    "        f.write(catalog_info_t)\n",
    "    upload_blob(bucket_name, \"catalog-info.yaml\", os.path.join(destination_prefix, \"catalog-info.yaml\"))\n",
    "\n",
    "\n",
    "save_yolo_model_op = components.create_component_from_func(\n",
    "    save_yolo_model,\n",
    "    base_image='python:3.8',\n",
    "    packages_to_install=['google-cloud-storage', 'six'],\n",
    "    output_component_file='save_yolo_model.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "43e198a9-04c4-41e2-9bf5-a685bf8f20e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:11:14.149915Z",
     "start_time": "2023-10-06T09:11:14.141897Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import kubernetes as k8s\n",
    "from kubernetes.client.models import V1EnvVar, V1ContainerPort\n",
    "\n",
    "pipeline_name = 'Train YOLOv7 with Labelstudio and 1 T4 GPU'\n",
    "pipeline_description = 'A pipeline to train YOLOv7 on a custom data set from Teknoir Labelstudio. Transfer learn from \"transfer_weights.\"'\n",
    "\n",
    "\n",
    "@dsl.pipeline(name=pipeline_name, description=pipeline_description)\n",
    "def train_yolov7(model_name: str,\n",
    "                 labelstudio_project: str,\n",
    "                 labels: list,\n",
    "                 namespace: str,\n",
    "                 domain: str,\n",
    "                 config_template_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/cfg/yolov7-tiny-config-template.yaml',\n",
    "                 transfer_weights_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/weights/yolov7_training.pt',\n",
    "                 hyp_file_url: str = 'gs://teknoir-ai.teknoir.cloud/yolov7/hyp/hyp.scratch.tiny.yaml',\n",
    "                 train_frac: float = 0.7,\n",
    "                 # fraction of samples to use for training (test_frac = 1 - train_frac - validate_frac)\n",
    "                 validate_frac: float = 0.2,\n",
    "                 # fraction of samples to use for validation (test_frac = 1 - train_frac - validate_frac)\n",
    "                 batch_size: int = 16,\n",
    "                 img_size_train: int = 416,\n",
    "                 img_size_test: int = 416,\n",
    "                 epochs: int = 300,\n",
    "                 ):\n",
    "    \"\"\"Train YOLOv7 with Labelstudio\"\"\"\n",
    "\n",
    "    gpu_instance_type = \"nvidia-tesla-t4\"  # https://cloud.google.com/compute/docs/gpus\n",
    "\n",
    "    prepare = prepare_labelstudio_data_for_yolo_op(\n",
    "        project=labelstudio_project,\n",
    "        labels=labels,\n",
    "        namespace=namespace,\n",
    "        domain=domain,\n",
    "        config_template_url=config_template_url,\n",
    "        transfer_weights_url=transfer_weights_url,\n",
    "        hyp_file_url=hyp_file_url,\n",
    "        train_frac=train_frac,\n",
    "        validate_frac=validate_frac\n",
    "    )\n",
    "    prepare.execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "    prepare.add_pod_annotation('proxy.istio.io/config', '{\"holdApplicationUntilProxyStarts\": true }')\n",
    "    prepare.add_pod_annotation('sidecar.istio.io/inject', 'true')\n",
    "\n",
    "    \n",
    "    train = train_op(\n",
    "        workspace_path=kfp.dsl.InputArgumentPath(argument=prepare.outputs['workspace'], path='/tmp/outputs/workspace/data'),\n",
    "        model_name=model_name,\n",
    "        weights=prepare.outputs['weights_file'],\n",
    "        data=prepare.outputs['data_file'],\n",
    "        config=prepare.outputs['config_file'],\n",
    "        hyp=prepare.outputs['hyp_file'],\n",
    "        batch_size=batch_size,\n",
    "        img_size_train=img_size_train,\n",
    "        img_size_test=img_size_test,\n",
    "        epochs=epochs)\n",
    "    train.after(prepare) \\\n",
    "        .add_port(V1ContainerPort(container_port=8099)) \\\n",
    "        .add_port(V1ContainerPort(container_port=8079)) \\\n",
    "        .set_gpu_limit(1) \\\n",
    "        .add_node_selector_constraint('cloud.google.com/gke-accelerator', gpu_instance_type) \\\n",
    "        .execution_options.caching_strategy.max_cache_staleness = \"P0D\"\n",
    "\n",
    "\n",
    "    save_model = save_yolo_model_op(\n",
    "        model_name=model_name,\n",
    "        namespace=namespace,\n",
    "        domain=domain,\n",
    "        description=pipeline_description,\n",
    "        best_model=train.outputs['best_model'],\n",
    "        last_model=train.outputs['last_model'],\n",
    "        names=train.outputs['names'],\n",
    "        data=train.outputs['data'],\n",
    "        config=train.outputs['config'],\n",
    "        hyp=train.outputs['hyp'],\n",
    "        f1_curve=train.outputs['F1_curve_image'],\n",
    "        p_curve=train.outputs['P_curve_image'],\n",
    "        pr_curve=train.outputs['PR_curve_image'],\n",
    "        r_curve=train.outputs['R_curve_image'],\n",
    "        results_txt=train.outputs['results_txt'],\n",
    "        results_image=train.outputs['results_image'],\n",
    "        confusion_matrix_image=train.outputs['confusion_matrix_image']\n",
    "    ).after(train)\n",
    "    save_model.execution_options.caching_strategy.max_cache_staleness = \"P0D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "927c619e-85ee-4b6d-a8ad-d85d8706bdf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-06T09:11:15.109557Z",
     "start_time": "2023-10-06T09:11:15.000532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "client = kfp.Client(namespace='teknoir')\n",
    "# 5h timeout\n",
    "pipeline_conf = kfp.dsl.PipelineConf().set_timeout(3600 * 5).set_image_pull_policy(policy=\"Always\")\n",
    "workflow = kfp.compiler.Compiler().compile(pipeline_func=train_yolov7,\n",
    "                                           package_path=\"train_yolov7.yaml\",\n",
    "                                           pipeline_conf=pipeline_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "99bcc356-7992-4a64-ba88-b1a3f0744c82",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to read a token from file '/var/run/secrets/kubeflow/pipelines/token' ([Errno 2] No such file or directory: '/var/run/secrets/kubeflow/pipelines/token').\n",
      "WARNING:root:Failed to set up default credentials. Proceeding without credentials...\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "import json\n",
    "\n",
    "pipeline_version_file = pipeline_file = 'train_yolov7.yaml'\n",
    "\n",
    "client = kfp.Client(namespace='teknoir')\n",
    "# 5h timeout\n",
    "pipeline_conf = kfp.dsl.PipelineConf().set_timeout(3600 * 5).set_image_pull_policy(policy=\"Always\")\n",
    "workflow = kfp.compiler.Compiler().compile(pipeline_func=train_yolov7,\n",
    "                                           package_path=pipeline_file,\n",
    "                                           pipeline_conf=pipeline_conf)\n",
    "\n",
    "filter = json.dumps({'predicates': [{'key': 'name',\n",
    "                                     'op': 1,\n",
    "                                     'string_value': pipeline_name}]})\n",
    "pipelines = client.pipelines.list_pipelines(filter=filter)\n",
    "\n",
    "if not pipelines.pipelines:\n",
    "    pipeline = client.pipeline_uploads.upload_pipeline(pipeline_file,\n",
    "                                                       name=pipeline_name,\n",
    "                                                       description=pipeline_description)\n",
    "else:\n",
    "    pipeline_version_name = pipeline_name + f' - {str(uuid.uuid4())[:6]}'\n",
    "    pipeline_version = client.pipeline_uploads.upload_pipeline_version(pipeline_version_file,\n",
    "                                                                       name=pipeline_version_name,\n",
    "                                                                       pipelineid=pipelines.pipelines[0].id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a50e42d-9007-4160-b695-3ca9d38f9112",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a1784f-572c-4a50-ac20-af4733a4e8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
